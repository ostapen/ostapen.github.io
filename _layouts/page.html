---
layout: default
---
<div class="container-fluid">

  <!-- <div>
    <h2>{{ page.title | escape }}</h2>
  </div> -->
<!-- <h2><b>Highlights</b></h2> -->

  
<div class="row">
  <div">
    {{ content }}
  </div>
  <div class="container">
    <h3><i><u>Johns Hopkins University’s Fifth Frederick Jelinek Language Technology Workshop</u></i></h3>
    <div class="column" style="column-count:2; column-width:360px">

      <!-- <div class="col-md-4" style="width: 18rem;"> -->
        <div class="card" style="width:22rem;">


          <img src='/images/Specia-Team-Pic.jpg' class="card-img-top" alt="...">
        <!-- <div class="card-footer w-100"> -->

            <p class="card-footer" style="width:25rem; text-align:left;">
               I was in <a href="https://www.imperial.ac.uk/people/l.specia">Dr. Lucia Specia’s</a> multimodal machine translation team (MMT). I helped
               develop deep, context-aware techniques which, given a captioned image, could use the image to
               resolve ambiguities when translating text. MMT was part of the <a href="https://srvk.github.io/jsalt-2018-grounded-s2s/">Grounded Sequence to Sequence Transduction team</a>,
               a larger effort to understand actions in instructional videos by aligning audio, visual data, and video transcriptions. (May-August 2018)

    
            </p>
            <!-- <p> We presented our work to industry sponsors and academic researchers at the conclusion of the workshop. *Multimodal Machine Translation* is around 1:45:00.  
              [![Grounded Sequence To Sequence Transduction Final Presentation](http://img.youtube.com/vi/7B4iRMd88I0/0.jpg)](http://www.youtube.com/watch?v=7B4iRMd88I0&t=6330s)  </p> -->

        </div>

        </div>
        <p><u>Relevant Work:</u><br>
        L. Specia, J. Wang, S.J. Lee, <i>A. Ostapenko</i>, P. Madyastha. "Read, Spot and Translate." Springer Machine Translation Journals, (accepted November 2019). To appear.</p>

        </div>
        <div class="container" style="padding-top:25px padding-bottom:25px">
          <h3><i><u>Applied Machine Learning Research with Vestigo Ventures</u></i></h3>
          <div class="column" style="column-count:2; column-width:360px">
            <!-- <div class="col-md-4" style="width: 18rem;"> -->
              <div class="card" style="width:25rem;">
              <!-- <div class="card-footer w-100"> -->
                  <p class="card-footer" style="width:20rem; text-align:left;">
                  From Spring 2019-2020, I worked on applied research projects with <a href="https://www.vestigoventures.com">Vestigo Ventures</a>, an early-stage
                venture capital firm. In my first project, I developed a versatile, ML-driven automatic website classification tool which allows Vestigo to identify new
              investment opportunities from the World Wide Web.</p>
                  <img src='/images/Vestigo.jpeg' class="card-img-top" alt="...">
              </div>

              </div>
              <p><u>Relevant Work:</u><br>
                <i>A. Ostapenko</i>, R. Neamtu, and F. Anderson. "FinDX: A Versatile, Low Resource Approach to Financial Website Classification." IEEE Big Data 2019, Third International Workshop on Big Data for Financial News and Data. To appear.
                <br><br><b>Oct-Dec 2019</b>: I am completed my senior capstone project at Vestigo Ventures and <a href="https://www.cogolabs.com">Cogo Labs</a>. I worked with two other WPI students to build a clickstream data analysis tool
                which Vestigo & Cogo can use to visualize and analyze how users navigate through company websites. Our tool draws upon concepts in computer science, mathematics, and industrial engineering.</p>
              </div>


            </div>


</div>
